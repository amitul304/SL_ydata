{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "svm_exercise.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amitul304/SL_ydata/blob/master/svm_exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "kHjry4MFiQZ6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Support Vector Machine Exercise\n",
        "In this exercise you will learn about:\n",
        "1. Implementing SVM from scratch using a sub-gradient method called Pegasos (2011).\n",
        "2. The effect of imbalance and non-seperable classes on the SVM solution.\n",
        "3. Pratical SVM in scikit-learn on a simple example including hyper parameter optimization wrapper class to find optimal regularization, loss and multiclass technique.\n",
        "4. Optional reading material on one class, new probability interpretation of SVM"
      ]
    },
    {
      "metadata": {
        "id": "JR8w86nqjxQe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1. Implementing the PEGASOS SVM\n",
        "We will implement the simplest SVM implementation. \n",
        "It is based on a paper by Shalev-Shwartz et al (see reading material below).\n",
        "The essense of the algorithm is copied below from the paper:\n",
        "![alt text](https://snag.gy/Gpi1Mk.jpg)\n",
        "\n",
        "Here is a little more deailed explanation (it's recommended to read the full paper).\n",
        "![alt_text](https://snag.gy/koA0ue.jpg)\n",
        "\n",
        "1. Remember the SVM primal optimization problem in the linearly separable case is defined as\n",
        "$$\n",
        "\\begin{align}\n",
        "\\min_{\\mathbf{w},b}\\quad &\\frac{\\|\\mathbf{w}\\|}{2}, \\\\\n",
        "s.t.\\quad&y^{(i)} (\\mathbf{w^T}\\mathbf{x}^{(i)}+b) \\ge 1 &\\forall i \\in \\{1,\\dots,N\\}\n",
        "\\end{align}\n",
        "$$\n",
        "Explain in words why in this case it is **not equivalent** to solve the original problem with a separate bias term $b$, and a reformulation of the problem using a padding of 1 to each sample (effectively ignoring the bias term in the original formulation, and letting the coordinate of $w$ which corresponds to the padding serve as the bias).\n",
        "\n",
        "2. Implement a class *PegasosSVM* which has parameter $\\lambda$ and $T$ and methods *fit* and *predict* and *decision_function* where the latter is the distance from the plane (aka model's output score).\n",
        "  * **although the above**, here you are required to pad the samples with 1, and not use bias (the separating hyperplane must go through the origin).\n",
        "  * don't forget labels should be {-1,1} - change them if it is {0,1}\n",
        "  * don't forget to normalize your features. You can use [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) and [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) to create a pipeline which first standartize features and then learn.\n",
        "\n",
        "3. test your class on the breast cancer database [load_breast_cancer](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html), and compare your results to the Native Scikit-learn implementation [LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC)\n",
        "  * you can use [`cross_val_score`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)\n",
        "\n",
        "4. Analyze the effect of the hyper parameter $\\lambda$ on your training and test error. \n",
        "  * you can use scikit-learn's `validation_curve`\n",
        "\n",
        "5. Analyze the learning curve (performance as function of training size)\n",
        "  * you can use scikit-learn's `learning_curve`\n",
        "\n",
        "6. *Bonus: Mini-batches* . In the paper, the authors summed the loss over several samples before updating. Extend your class to support mini-batches and analyze the perfomance effect if exists. See more details below\n",
        "\n",
        "![](https://i.ibb.co/0BGwVz7/1.png)"
      ]
    },
    {
      "metadata": {
        "id": "PVj66a0uEwoI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2. The importance of class weighting in SVM\n",
        "When classes are imbalance and not seperable, SVM might result in non intuitive solution.\n",
        "\n",
        "To see this create a function `make_data` which generates a two dimensional dataset with `n_samples=1000` where a fraction `imbalance=0.1` is labeled `0` and the rest `1`. The distribution of $x$ given $y=0$ or $y=1$ is gaussian with std 0.5 or 2, respectively and that the centers are at $x_1=0$ and $x_2=-d/2$ and $x_2=d/2$, respectively. In other words,  \n",
        "$$\n",
        "p(x|y) = \\mathcal{N}(\\mu=(±d/2,0),\\,\\sigma=1/2+3/2\\times y)\n",
        "$$\n",
        "and \n",
        "$$\n",
        "p(y=0) = 0.1\n",
        "$$. \n",
        "\n",
        "* For `d=10` and `d=2` plot the scatter plot of the data. Where would you think the SVM hyperplane will lie?\n",
        "\n",
        "* Now, for each of these d, draw on top of the scatter the seperation hyperplane of the built-in LinearSVC in scikit-learn.\n",
        "\n",
        "To plot the hyperplane, note that the fitted model has the `coef_` and `intercept_` properties.\n",
        "Add also the `accuracy` and the `balanced_accuracy` metrics to the plot title.\n",
        "\n",
        "* repeat the experiment but now set the `class_weight` to be `balanced`. \n",
        "\n",
        "Explain what was the problem and how changing class weight solved it.\n"
      ]
    },
    {
      "metadata": {
        "id": "O-RXyr-hJQt7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#3. SVM hyperparameter search\n",
        "Machine learning pipelines in general and models in particular has several hyperparametrs that we currently do not know how to optimize.\n",
        "Instead, researcher use grid search or random search techniques to find the optimal hyper parameters.\n",
        "In this exercise you are requested to compare between default params model and a model which internally optimize for these hyper parameters.\n",
        "Luckily, scikit-learn already has a ready class for doing that called `GridSearchCV`. Use the latter class to check all the combination of the following hyper parameters:\n",
        "* `'multi_class':['ovr', 'crammer_singer']`\n",
        "* `'loss':['hinge', 'squared_hinge']`\n",
        "* `'C': np.logspace(-3,3,10)`\n",
        "\n",
        "Compare the accuracy of the model to a default `LinearSVC` model with no hyper parameter optimization in terms of accuracy.\n",
        "\n",
        "Note: Recall, that hyper parameter search is a learning procedure by itself, thus one should not use test data for the process. \n",
        "\n",
        "To do this you can use the handy `cross_val_score` function.\n"
      ]
    },
    {
      "metadata": {
        "id": "N-5bHUGQitCY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 4. Optional Reading Materials\n",
        "1. Shalev-Shwartz, S., Singer, Y., Srebro, N., & Cotter, A. (2011). Pegasos: Primal estimated sub-gradient solver for svm. Mathematical programming, 127(1), 3-30. [[pdf](http://www.ee.oulu.fi/research/imag/courses/Vedaldi/ShalevSiSr07.pdf)]\n",
        "\n",
        "2. Schölkopf, B., Williamson, R. C., Smola, A. J., Shawe-Taylor, J., & Platt, J. C. (2000). Support vector method for novelty detection. In Advances in neural information processing systems (pp. 582-588). [[pdf](http://papers.nips.cc/paper/1723-support-vector-method-for-novelty-detection.pdf)]\n",
        "\n",
        "3. Livni, R., Crammer, K. & Globerson, A.. (2012). A Simple Geometric Interpretation of SVM using Stochastic Adversaries. Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics, in PMLR 22:722-730. [[pdf](http://proceedings.mlr.press/v22/livni12/livni12.pdf)]\n",
        "\n"
      ]
    }
  ]
}